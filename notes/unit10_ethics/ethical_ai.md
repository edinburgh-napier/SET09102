---
title: Ethical AI
parent: Ethics
has_children: true
has_toc: false
nav_order: 5
---


# Ethical Use of AI in Software Systems

The ethical use of AI in software systems is an increasingly important consideration as AI technologies 
are integrated into many aspects of daily life. AI systems have the potential to transform industries, 
improve decision-making, and enhance user experiences. However, with these opportunities come ethical 
challenges, as AI systems can impact fairness, privacy, accountability, and transparency. Ethical AI 
seeks to ensure that these systems are developed and deployed responsibly, respecting human rights and 
minimising potential harms to individuals and society.

Ethical AI requires a balance between innovation and responsibility, where AI systems are not only designed 
to perform effectively but also align with societal values and legal standards. Key principles in ethical 
AI include **fairness** (avoiding bias and discrimination), **transparency** (making AI decisions 
understandable), **accountability** (ensuring human responsibility for AI outcomes), and **privacy** 
(safeguarding users’ personal data). Adhering to these principles is critical, as AI systems can sometimes 
reflect or amplify existing social biases, produce opaque outcomes, or inadvertently compromise privacy.

As regulatory bodies worldwide develop frameworks and laws to address these challenges, software 
engineers must stay informed and incorporate ethical considerations into their AI projects. International 
standards, such as the EU’s AI Act, along with guidelines from institutions like the UK Government and 
the Organisation for Economic Co-operation and Development (OECD), provide frameworks that guide ethical 
AI development. By following these guidelines, software engineers can develop AI systems that are 
trustworthy, safe, and aligned with ethical standards, ultimately benefiting users and fostering public 
trust in AI technology.

## Legal frameworks

The current status of legal frameworks for the ethical use of AI, while advancing, remains fragmented 
and faces several challenges in addressing the complexities of rapidly evolving AI technologies. 
Regulations such as the EU’s [AI Act](https://eur-lex.europa.eu/eli/reg/2024/1689/oj), the UK’s 
pro-innovation [AI guidelines](https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#executive-summary), 
and Canada’s [Artificial Intelligence and Data Act](https://ised-isde.canada.ca/site/innovation-better-canada/en/artificial-intelligence-and-data-act) 
(AIDA) represent significant steps toward creating accountability, promoting transparency, and protecting 
human rights in AI. However, these frameworks vary widely in approach and scope, leading to inconsistencies 
that complicate compliance for global AI developers. For instance, the EU’s AI Act adopts a stringent, 
risk-based approach that categorises AI applications by their potential societal impact, whereas the UK’s 
approach is more flexible, allowing sector-specific regulators to craft tailored guidelines. This 
divergence reflects differing priorities — strict regulatory oversight versus innovation facilitation — 
which can create challenges for multinational companies trying to navigate these legal landscapes.

Moreover, current legal frameworks often struggle to keep pace with the speed of AI advancements, leading 
to regulatory gaps in emerging areas such as generative AI, autonomous systems, and adaptive learning 
models. AI regulations typically focus on transparency, privacy, and fairness, but the technical specifics 
of enforcing these principles—such as explainability in complex machine learning models—are still 
challenging to codify in law. This gap risks creating “grey areas” where AI systems may operate without 
clear accountability.

Another critical limitation is that many existing frameworks lack robust enforcement mechanisms, making it 
difficult to hold organisations accountable when ethical violations occur. For example, without a 
standardised global enforcement authority, companies can face different levels of scrutiny and consequences 
depending on where they operate. Additionally, while current frameworks emphasise principles such as 
fairness and accountability, operationalising these values in technical terms remains a challenge, as 
it is often unclear how to apply high-level ethical concepts in specific AI use cases.

The absence of unified, global standards and enforceable guidelines makes it difficult to ensure a 
consistent ethical foundation for AI across borders. While localised legal frameworks are valuable, 
their variability underscores the need for collaboration and harmonisation on an international scale. 
As a result, the current state of AI regulation provides a starting point for ethical governance but 
still requires further development to effectively guide AI's role in society, protect individuals, and 
adapt to new technological realities.

## EU regulation

The EU's Artificial Intelligence Act (AI Act) is currently one of the most comprehensive legal frameworks 
for AI, particularly in its ambition to regulate AI across multiple dimensions, with a broad and 
well-defined approach to ethics, safety, and accountability. 

### Risk-Based Approach

The AI Act introduces a risk-based classification system for AI systems, categorising them into four 
levels: 

* Unacceptable
* High
* Limited
* Minimal

This approach allows for targeted regulation, ensuring that the most potentially harmful applications — 
such as AI used in critical sectors like healthcare, law enforcement, and employment — are subject to 
the strictest oversight. This dynamic, tiered system is unique and is designed to focus resources and 
regulatory efforts where they are most needed.

### Comprehensive Scope

Unlike previous AI regulations in other jurisdictions which may focus on specific industries or 
technologies, the EU's AI Act has a wide-reaching scope. It covers all AI systems deployed in the EU, 
regardless of their origin, making it the first regulation to attempt to regulate AI holistically. The 
Act applies to public and private sector AI applications alike, offering a comprehensive framework for 
AI governance, which includes elements such as data governance, algorithmic transparency, and human 
oversight.

### Focus on High-Risk AI Systems

The EU's AI Act places a strong emphasis on high-risk AI systems, which must comply with specific 
requirements like rigorous risk assessments, data quality standards, documentation and transparency, 
and human oversight. This is particularly relevant in sensitive areas such as criminal justice, 
healthcare, and recruitment, where AI has the potential to directly affect individuals' rights, 
safety, and well-being. These regulations ensure that AI systems in these areas undergo regular 
scrutiny and certification, offering a high degree of consumer protection.

### Prohibition of Unacceptable Risks

The AI Act prohibits certain AI applications deemed to pose unacceptable risks, such as AI used for 
mass surveillance or social scoring. By taking a strong stance on these high-risk areas, the EU 
demonstrates its commitment to preventing AI systems that could infringe upon fundamental rights or 
enable systemic harm. This prohibition is one of the more stringent regulatory aspects in the global 
landscape.

### Emphasis on Transparency and Accountability

The EU Act sets out clear and enforceable requirements for AI systems to provide transparent operations. 
These include making the decisions of high-risk AI systems explainable and accessible to users, and 
ensuring there is adequate human oversight. Furthermore, AI developers must maintain records that 
document how the system was designed, how it operates, and its impacts. This level of accountability 
and traceability is unprecedented in many other AI regulatory frameworks.

### Global Applicability

The EU's AI Act is not only groundbreaking in terms of its thorough regulation within Europe but also in 
its extraterritorial reach. It applies to all entities that deploy AI within the EU, regardless of where 
the company is based. This global applicability forces companies worldwide to comply with EU standards 
if they want to access the European market, positioning the AI Act as a potential global standard for AI 
regulation.

### Human Rights and Ethical Standards

The AI Act places a strong emphasis on human rights, aligning with broader EU values such as democracy, 
dignity, and the protection of personal data. Its focus on fairness, transparency, non-discrimination, 
and accountability in AI systems is a major step toward ensuring AI development and deployment align with 
ethical standards. This approach makes the AI Act more aligned with human rights principles compared to 
some other regulations, which may focus primarily on safety or economic factors.

### Penalties for Non-Compliance

The AI Act includes substantial penalties for non-compliance, with fines up to €30 million or 6% of a 
company's global annual turnover, whichever is higher. These penalties are among the most severe for 
technology regulation and reinforce the importance of compliance. This enforcement mechanism makes the 
AI Act one of the most robust in terms of ensuring adherence to its provisions.

### Continuous Evaluation and Updates

The AI Act includes provisions for ongoing review and updates to ensure it remains relevant as technology 
evolves. AI is a fast-moving field, and the regulation has been designed to be flexible enough to adapt to 
emerging developments, maintaining its relevance over time. This forward-looking approach makes it a more 
comprehensive and sustainable framework compared to others that may struggle to keep pace with AI innovation.

The EU's AI Act is the most comprehensive AI regulation currently available due to its broad and detailed 
coverage, its robust ethical focus, and its global applicability. It combines clear risk-based regulation 
with detailed safety, accountability, and human rights provisions, addressing both the immediate 
challenges and the long-term concerns of AI technologies. While it sets a high bar for other jurisdictions, 
the ongoing challenge will be its enforcement, ensuring that the rules are effective across diverse 
applications and industries globally.

## Explainable AI

Explainable AI (XAI) is an active area of AI research focused on making AI systems and their 
decision-making processes understandable to humans. As AI becomes increasingly embedded in everyday 
applications — ranging from healthcare and finance to autonomous vehicles and criminal justice — there 
is a growing need for transparency, so that users, stakeholders, and regulators can understand how and 
why AI systems reach certain conclusions. Explainable AI addresses this by providing tools and techniques 
that make AI decisions interpretable, reducing the "black box" nature often associated with complex 
models like deep learning. By enhancing transparency, XAI promotes trust, accountability, and ethical 
responsibility in AI applications, helping developers and organisations ensure that their systems align 
with legal requirements, societal expectations, and end-user needs. 

There are several main approaches to achieving transparency and interpretability in AI systems. One key 
method is **post-hoc explainability**, where explanations are generated after a model has made its 
predictions, allowing developers to understand the reasoning without altering the model itself. 
Techniques in this category include visualisation tools, which highlight features or input areas that 
influenced a decision, and methods such as 
[SHAP](https://xai-tutorials.readthedocs.io/en/latest/_model_agnostic_xai/shap.html) (Shapley Additive 
Explanations) and 
[LIME](https://xai-tutorials.readthedocs.io/en/latest/_model_agnostic_xai/lime.html) (Local Interpretable 
Model-agnostic Explanations), which approximate how different inputs impact predictions.

Another approach is **model-intrinsic interpretability**, where the AI model is designed to be inherently 
understandable. This can involve using simpler, inherently interpretable algorithms, such as 
[decision trees](https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/) or 
[linear regression](https://www.analyticsvidhya.com/blog/2022/01/an-introductory-note-on-linear-regression/), 
which naturally provide insights into their decision-making processes. Although these models may not 
achieve the same accuracy as complex deep learning models, they offer a clear and direct interpretation 
of how inputs lead to outputs.

Lastly, **model simplification** seeks to balance performance and interpretability by approximating a 
complex model with a simpler one that behaves similarly but is easier to understand. This may involve 
using [rule-based systems](https://www.scaler.com/topics/artificial-intelligence-tutorial/rule-based-system-in-ai/) 
or [surrogate models](https://doi.org/10.48550/arXiv.2406.16626) that mimic the behaviour of a complex 
model, providing insights into its decisions without sacrificing interpretability. These approaches 
together form the foundation of explainable AI; however, a full discussion of the details of XAI is beyond
the scope of this module.

{: .tip-title }
> [<i class="fa-regular fa-lightbulb"></i> Practical tips for the ethical use of AI](ethical_ai_tips)

